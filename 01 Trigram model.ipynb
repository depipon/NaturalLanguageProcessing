{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47851042",
   "metadata": {},
   "source": [
    "# Develop trigram model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90a2552",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0385d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e09751",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c2f87221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sampled JSON files: 20\n",
      "Snippet of first doc:\n",
      " alzheimer's disease and other types of dementia are the top cause for disabilities in later life and various types of experiments have been performed to understand the underlying mechanisms of the disease with the aim of coming up with potential drug targets. these experiments have been carried out  ...\n"
     ]
    }
   ],
   "source": [
    "# Set the path to your dataset directory\n",
    "path = os.path.expanduser(\"/Users/davepipon/Desktop/DS397 Data/coleridgeinitiative-show-us-the-data/train/*.json\")\n",
    "\n",
    "# Load training file names\n",
    "files = glob.glob(path)\n",
    "\n",
    "# Randomly select 20 files\n",
    "sample_files = random.sample(files, min(20, len(files)))\n",
    "\n",
    "# Read and store randomly sampled documents\n",
    "sample_data = []\n",
    "for path in sample_files:\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "        for entry in data:\n",
    "            sample_data.append(entry.get(\"text\", \"\"))\n",
    "\n",
    "print(\"Number of sampled JSON files:\", len(sample_files))\n",
    "print(\"Snippet of first doc:\\n\", sample_data[0][:300], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccbbc61",
   "metadata": {},
   "source": [
    "### Set up training and heldout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77fedb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training corpus snippet:\n",
      " Yu et al. investigated metabolic changes in the urine of APP/ PS1 transgenic mice prior to cognitive impairment (132) . At 2 months of age, the spatial working memory of APP/PS1 mice showed no significant differences when compared to NTG controls (132) . However, metabolomics analysis of urine from  ...\n",
      "Heldout corpus snippet:\n",
      " Approximately 41 percent of financially independent undergraduates received financial aid, and the average amount they received was about $3,500. More than one-third of independent students (36 percent) received grants (averaging about $2,000), and 18 percent received loans (averaging $3,500) (table ...\n"
     ]
    }
   ],
   "source": [
    "# Split data to train and heldout sets\n",
    "def train_heldout_split(corpus, heldout_ratio=0.1, seed=42):\n",
    "    random.seed(seed)\n",
    "    random.shuffle(corpus)\n",
    "    split_idx = int(len(corpus) * (1 - heldout_ratio))\n",
    "    train = corpus[:split_idx]\n",
    "    heldout = corpus[split_idx:]\n",
    "    return train, heldout\n",
    "\n",
    "# Implement splitting function\n",
    "sample_train, sample_heldout = train_heldout_split(sample_data, heldout_ratio=0.1)\n",
    "\n",
    "# Aggregate training and heldout corpora into single strings\n",
    "train_corpus = \" \".join(sample_train)\n",
    "heldout_corpus = \" \".join(sample_heldout)\n",
    "\n",
    "# Check text content\n",
    "print(\"Training corpus snippet:\\n\", train_corpus[:300], \"...\")\n",
    "print(\"Heldout corpus snippet:\\n\", heldout_corpus[:300], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cc0cca",
   "metadata": {},
   "source": [
    "### Set up test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9700e236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sampled test JSON files: 10\n",
      "Snippet of first test doc:\n",
      " Scales with varying degrees of measurement reliability are often used in the context of multistage sampling, where variance exists at multiple levels of analysis (e.g., individual and group). Because methodological guidance on assessing and reporting reliability at multiple levels of analysis is cur ...\n"
     ]
    }
   ],
   "source": [
    "# Generate test data outside initial sample data\n",
    "test_path = os.path.expanduser(\"/Users/davepipon/Desktop/DS397 Data/coleridgeinitiative-show-us-the-data/train/*.json\")\n",
    "\n",
    "# Remove files already in sample_files\n",
    "sample_file_set = set(sample_files)\n",
    "all_test_files = glob.glob(test_path)\n",
    "test_files = [f for f in all_test_files if f not in sample_file_set]\n",
    "\n",
    "# Randomly select 10 test files\n",
    "test_sample_files = random.sample(test_files, min(10, len(test_files)))\n",
    "test_data = []\n",
    "for path in test_sample_files:\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "        for entry in data:\n",
    "            test_data.append(entry.get(\"text\", \"\"))\n",
    "\n",
    "# Aggregate test corpus into a single string\n",
    "test_corpus = \" \".join(test_data)\n",
    "\n",
    "print(\"Number of sampled test JSON files:\", len(test_sample_files))\n",
    "print(\"Snippet of first test doc:\\n\", test_corpus[:300], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf650074",
   "metadata": {},
   "source": [
    "## Develop algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac231291",
   "metadata": {},
   "source": [
    "### Set up sentence segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "408a88ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> <s> Yu et al. </s>', '<s> <s> investigated metabolic changes in the urine of APP/ PS1 transgenic mice prior to cognitive impairment (132) . </s>', '<s> <s> At 2 months of age, the spatial working memory of APP/PS1 mice showed no significant differences when compared to NTG controls (132) . </s>']\n"
     ]
    }
   ],
   "source": [
    "# Sentence splitting using NLTK with markers for sentence boundaries\n",
    "def sentence_split(corpus,n=3):\n",
    "    sent = sent_tokenize(corpus)\n",
    "    start = \"<s> \" * (n-1)\n",
    "    end = \" </s>\"\n",
    "    sentences = [start + \" \" + s + \" \" + end for s in sent]\n",
    "    sentences_clean = [re.sub(r'\\s+', ' ', s).strip() for s in sentences]\n",
    "    return sentences_clean\n",
    "\n",
    "# Test\n",
    "print(sentence_split(train_corpus)[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a875936f",
   "metadata": {},
   "source": [
    "### Set up N-gram counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c79da0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample unigram counts: [('Yu', 2), ('et', 352), ('al.', 142), ('investigated', 11), ('metabolic', 89)]\n",
      "Sample bigram counts: [(('<s>', 'Yu'), 1), (('Yu', 'et'), 2), (('et', 'al.'), 141), (('al.', '</s>'), 142), (('<s>', 'investigated'), 3)]\n",
      "Sample trigram counts: [(('<s>', '<s>', 'Yu'), 1), (('<s>', 'Yu', 'et'), 1), (('Yu', 'et', 'al.'), 2), (('et', 'al.', '</s>'), 141), (('<s>', '<s>', 'investigated'), 3)]\n"
     ]
    }
   ],
   "source": [
    "# Set up N-gram counters\n",
    "def n_gram_counter(corpus, n=3):\n",
    "\n",
    "    unigram_counts = Counter()\n",
    "    bigram_counts = Counter()\n",
    "    trigram_counts = Counter()\n",
    "    sentences = sentence_split(corpus,n=n)\n",
    "\n",
    "    for sent in sentences:\n",
    "        tokens = sent.split()\n",
    "\n",
    "        for i, tok in enumerate(tokens):\n",
    "            # Unigrams\n",
    "            if tok not in {\"<s>\", \"</s>\"}: # skip <s> and </s>\n",
    "                unigram_counts[tok] += 1\n",
    "\n",
    "            # Bigrams\n",
    "            if i > 0:\n",
    "                w1, w2 = tokens[i-1], tok\n",
    "                if not (w1 == \"<s>\" and w2 == \"<s>\"): # skip <s>, <s>\n",
    "                    bigram_counts[tokens[i-1], tok] += 1\n",
    "\n",
    "            # Trigrams\n",
    "            if i > 1:\n",
    "                trigram_counts[tokens[i-2], tokens[i-1], tok] += 1\n",
    "\n",
    "    return unigram_counts, bigram_counts, trigram_counts\n",
    "\n",
    "# Test\n",
    "unigram_counts, bigram_counts, trigram_counts = n_gram_counter(train_corpus)\n",
    "print(\"Sample unigram counts:\", list(unigram_counts.items())[:5])\n",
    "print(\"Sample bigram counts:\", list(bigram_counts.items())[:5])\n",
    "print(\"Sample trigram counts:\", list(trigram_counts.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a8550c",
   "metadata": {},
   "source": [
    "### Interpolate probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5fa9dc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(the|of,in) = 0.06311815745025817\n"
     ]
    }
   ],
   "source": [
    "# Implement add-k smoothing for interpolated trigram model\n",
    "def interpolated_prob(w, h1, h2, unigram, bigram, trigram, vocab_size, lambdas, add_k=1e-5):\n",
    "\n",
    "    lambda_tri, lambda_bi, lambda_uni = lambdas\n",
    "    \n",
    "    # unigram MLE with add-k\n",
    "    uni_count = unigram.get(w, 0) + add_k\n",
    "    uni_denom = sum(unigram.values()) + add_k * vocab_size\n",
    "    p_uni = uni_count / uni_denom\n",
    "\n",
    "    # bigram P(w|h2) = count(h2,w)/count(h2) smoothed with add_k\n",
    "    bi_num = bigram.get((h2, w), 0) + add_k\n",
    "    bi_den = unigram.get(h2, 0) + add_k * vocab_size\n",
    "    p_bi = bi_num / bi_den\n",
    "\n",
    "    # trigram P(w|h1,h2) = count(h1,h2,w)/count(h1,h2)\n",
    "    tri_num = trigram.get((h1, h2, w), 0) + add_k\n",
    "    tri_den = bigram.get((h1, h2), 0) + add_k * vocab_size\n",
    "    p_tri = tri_num / tri_den\n",
    "\n",
    "    return lambda_tri * p_tri + lambda_bi * p_bi + lambda_uni * p_uni\n",
    "\n",
    "# Test\n",
    "vocab = set(unigram_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "lambdas = (0.7, 0.2, 0.1)\n",
    "test_word = \"the\"\n",
    "test_h1 = \"of\"\n",
    "test_h2 = \"in\"\n",
    "prob = interpolated_prob(test_word, test_h1, test_h2, unigram_counts, bigram_counts, trigram_counts, vocab_size, lambdas)\n",
    "print(f\"P({test_word}|{test_h1},{test_h2}) =\", prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eb1a39",
   "metadata": {},
   "source": [
    "### Tune lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "54f8e8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grid lambdas: 100%|██████████| 231/231 [10:33<00:00,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambdas: (0.25, 0.4, 0.35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Grid search to tune lambdas\n",
    "def tune_lambdas(heldout_corpus, unigram, bigram, trigram, vocab, grid_step=0.05):\n",
    "        \n",
    "    sentences = sentence_split(heldout_corpus)\n",
    "    vocab_size = len(vocab)\n",
    "    best = None\n",
    "    best_ll = -float(\"inf\")\n",
    "\n",
    "    # Generate lambda triplets\n",
    "    lambdas_list = []\n",
    "    steps = np.arange(0, 1 + 1e-9, grid_step)\n",
    "    for l_tri in steps:\n",
    "        for l_bi in steps:\n",
    "            if l_tri + l_bi <= 1.0 + 1e-9:\n",
    "                l_uni = 1.0 - l_tri - l_bi\n",
    "                lambdas_list.append((l_tri, l_bi, l_uni))\n",
    "\n",
    "    # Evaluate each lambda set\n",
    "    for lambdas in tqdm(lambdas_list, desc=\"grid lambdas\"):\n",
    "        ll = 0.0\n",
    "        N = 0\n",
    "        for sent in sentences:\n",
    "            tokens = sent.split()\n",
    "            for i in range(2, len(tokens)):\n",
    "                w = tokens[i]\n",
    "                h1 = tokens[i - 2]\n",
    "                h2 = tokens[i - 1]\n",
    "                p = interpolated_prob(w, h1, h2, unigram, bigram, trigram, vocab_size, lambdas)\n",
    "                if p > 0:\n",
    "                    ll += math.log(p)\n",
    "                    N += 1\n",
    "        if N > 0:\n",
    "            ll_per_word = ll / N\n",
    "            if ll_per_word > best_ll:\n",
    "                best_ll = ll_per_word\n",
    "                best = lambdas\n",
    "\n",
    "    return best if best else (0.33, 0.33, 0.34)\n",
    "\n",
    "\n",
    "# Test\n",
    "vocab = set(unigram_counts.keys())\n",
    "best_lambdas = tune_lambdas(heldout_corpus, unigram_counts, bigram_counts, trigram_counts, vocab)\n",
    "print(\"Best lambdas:\", best_lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1605740",
   "metadata": {},
   "source": [
    "## Generate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e23b9c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sentence: However, an cortical thickness is a testament to Years diagnosis, which these risks and limitations of out-of-district age reflect differing generations of AD. patients. all (0.07) circles that methyl path\n",
      "Generated sentence: It has provided written informed consent. not 61.5% . of the Gulf of blurring stream-wise dynamics 56 subjects as an 3 brain atlas. (20.0%). found to 5 with in 1956).\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(unigram, bigram, trigram, vocab, lambdas, max_len=30):\n",
    "    vocab_list = sorted(vocab)\n",
    "    vocab_size = len(vocab_list)\n",
    "    prob_cache = {}\n",
    "\n",
    "    sent = ['<s>', '<s>']\n",
    "    for _ in range(max_len):\n",
    "        h1, h2 = sent[-2], sent[-1]\n",
    "        key = (h1, h2)\n",
    "\n",
    "        if key not in prob_cache:\n",
    "            probs = np.array([\n",
    "                interpolated_prob(w, h1, h2, unigram, bigram, trigram, vocab_size, lambdas)\n",
    "                for w in vocab_list\n",
    "            ])\n",
    "            probs = probs / probs.sum()\n",
    "            prob_cache[key] = probs\n",
    "\n",
    "        probs = prob_cache[key]\n",
    "        w = np.random.choice(vocab_list, p=probs)\n",
    "\n",
    "        if w == '</s>':\n",
    "            break\n",
    "        sent.append(w)\n",
    "\n",
    "    output = [t for t in sent if t not in ('<s>', '</s>')]\n",
    "    return ' '.join(output)\n",
    "\n",
    "# Generate sample sentences\n",
    "for _ in range(2):\n",
    "    sentence = generate_sentence(unigram_counts, bigram_counts, trigram_counts, vocab, best_lambdas)\n",
    "    print(\"Generated sentence:\", sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb8f8b",
   "metadata": {},
   "source": [
    "## Calculate perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2bc0f54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of sampled test files:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/davepipon/Desktop/DS397 Data/coleridgeinitiative-show-us-the-data/train/e72aa200-5e2b-4775-b155-d983cdf46f46.json',\n",
       " '/Users/davepipon/Desktop/DS397 Data/coleridgeinitiative-show-us-the-data/train/cf0223e8-f25e-4ea4-b63e-e971d72a9caa.json',\n",
       " '/Users/davepipon/Desktop/DS397 Data/coleridgeinitiative-show-us-the-data/train/a1043cef-47a8-4865-97dd-1e284c3ef160.json',\n",
       " '/Users/davepipon/Desktop/DS397 Data/coleridgeinitiative-show-us-the-data/train/68878dd8-162e-4dbd-a595-ca959dabe09b.json',\n",
       " '/Users/davepipon/Desktop/DS397 Data/coleridgeinitiative-show-us-the-data/train/fbcafd40-a96c-4f2e-b9ad-94937ba02cb3.json',\n",
       " '/Users/davepipon/Desktop/DS397 Data/coleridgeinitiative-show-us-the-data/train/796f35c1-ba6b-4552-8a7f-5d8b61164fb0.json',\n",
       " '/Users/davepipon/Desktop/DS397 Data/coleridgeinitiative-show-us-the-data/train/3570db8e-7968-4dc6-a080-c2a62a629e5d.json',\n",
       " '/Users/davepipon/Desktop/DS397 Data/coleridgeinitiative-show-us-the-data/train/c5d0d6b0-692c-495d-9382-31fbaba4ff22.json',\n",
       " '/Users/davepipon/Desktop/DS397 Data/coleridgeinitiative-show-us-the-data/train/694987db-3100-4c92-873c-02904ef71eef.json',\n",
       " '/Users/davepipon/Desktop/DS397 Data/coleridgeinitiative-show-us-the-data/train/08f936f9-5550-4085-924e-a483dd9e35d2.json']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on test set: 2021.0180522874603\n"
     ]
    }
   ],
   "source": [
    "def perplexity(corpus, unigram, bigram, trigram, vocab, lambdas):\n",
    "    sentences = sentence_split(corpus)\n",
    "    docs_tokenized = [s.split() for s in sentences]\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    log_prob = 0.0\n",
    "    N = 0\n",
    "    \n",
    "    for doc in docs_tokenized:\n",
    "        sent = []\n",
    "        for tok in doc:\n",
    "            sent.append(tok)\n",
    "            if tok in ('.', '!', '?'):\n",
    "                tokens = ['<s>', '<s>'] + sent + ['</s>']\n",
    "                for i in range(2, len(tokens)):\n",
    "                    w = tokens[i]\n",
    "                    h1, h2 = tokens[i-2], tokens[i-1]\n",
    "                    p = max(interpolated_prob(w, h1, h2, unigram, bigram, trigram, vocab_size, lambdas), 1e-12)\n",
    "                    log_prob += math.log(p)\n",
    "                    N += 1\n",
    "                sent = []\n",
    "        \n",
    "        # Handle leftover (no punctuation)\n",
    "        if sent:\n",
    "            tokens = ['<s>', '<s>'] + sent + ['</s>']\n",
    "            for i in range(2, len(tokens)):\n",
    "                w = tokens[i]\n",
    "                h1, h2 = tokens[i-2], tokens[i-1]\n",
    "                p = max(interpolated_prob(w, h1, h2, unigram, bigram, trigram, vocab_size, lambdas), 1e-12)\n",
    "                log_prob += math.log(p)\n",
    "                N += 1\n",
    "\n",
    "    return math.exp(-log_prob / N) if N > 0 else float('inf')\n",
    "\n",
    "# Compute perplexity score on test set\n",
    "print(\"List of sampled test files:\")\n",
    "display(test_sample_files)\n",
    "vocab = set(unigram_counts.keys())\n",
    "pp = perplexity(test_corpus, unigram_counts, bigram_counts, trigram_counts, vocab, best_lambdas)\n",
    "print(\"Perplexity on test set:\", pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da1c5f1",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e139ee7",
   "metadata": {},
   "source": [
    "Due to machine limitation, only 20 samples are selected for training and heldout. This exercise highlights the limitation of n-gram language models with limited number of training datasets. This is shown by the high perplexity value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
