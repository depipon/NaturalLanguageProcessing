{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7259f6",
   "metadata": {},
   "source": [
    "# FastText Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a50749c",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d005b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from gensim.models import FastText\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34e9a37",
   "metadata": {},
   "source": [
    "## Identify file directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a83270",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dir = \"/Users/davepipon/Desktop/DS397 Data/coleridgeinitiative-show-us-the-data/train/\"\n",
    "train_csv = \"/Users/davepipon/Desktop/DS397 Data/coleridgeinitiative-show-us-the-data/train.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e2db4a",
   "metadata": {},
   "source": [
    "## Develop Custom FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46461e",
   "metadata": {},
   "source": [
    "### Pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c396c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_documents(docs_dir, train_csv=None, n_docs=1000):\n",
    "    \"\"\"Load and preprocess up to n_docs documents for training.\"\"\"\n",
    "    documents = []\n",
    "    processed_sentences = []\n",
    "\n",
    "    print(f\"Loading {n_docs} documents...\")\n",
    "\n",
    "    if train_csv and os.path.exists(train_csv):\n",
    "        df = pd.read_csv(train_csv).fillna(\"\")\n",
    "        df = df.sample(n=min(n_docs, len(df)), random_state=42)\n",
    "\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Loading from CSV\"):\n",
    "            pub_id = row[\"Id\"]\n",
    "            json_path = os.path.join(docs_dir, f\"{pub_id}.json\")\n",
    "            if os.path.exists(json_path):\n",
    "                try:\n",
    "                    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        doc = json.load(f)\n",
    "                    text = extract_text_from_json(doc)\n",
    "                    if text.strip():\n",
    "                        documents.append(text)\n",
    "                except:\n",
    "                    continue\n",
    "    else:\n",
    "        json_files = [f for f in os.listdir(docs_dir) if f.endswith(\".json\")][:n_docs]\n",
    "        for filename in tqdm(json_files, desc=\"Loading from directory\"):\n",
    "            json_path = os.path.join(docs_dir, filename)\n",
    "            try:\n",
    "                with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    doc = json.load(f)\n",
    "                text = extract_text_from_json(doc)\n",
    "                if text.strip():\n",
    "                    documents.append(text)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "    # Preprocess into sentences with 3+ character words\n",
    "    for doc in tqdm(documents, desc=\"Preprocessing\"):\n",
    "        sentences = doc.split(\".\")\n",
    "        for sentence in sentences:\n",
    "            tokens = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", sentence.lower()).split()\n",
    "            tokens = [t for t in tokens if len(t) >= 3]\n",
    "            if len(tokens) >= 3:\n",
    "                processed_sentences.append(tokens)\n",
    "\n",
    "    print(f\"Created {len(processed_sentences)} sentences for training\")\n",
    "    return processed_sentences\n",
    "\n",
    "\n",
    "def extract_text_from_json(doc):\n",
    "    \"\"\"Recursively extract text from JSON document.\"\"\"\n",
    "    texts = []\n",
    "\n",
    "    def recurse(x):\n",
    "        if isinstance(x, dict):\n",
    "            for v in x.values():\n",
    "                recurse(v)\n",
    "        elif isinstance(x, list):\n",
    "            for v in x:\n",
    "                recurse(v)\n",
    "        elif isinstance(x, str):\n",
    "            texts.append(x)\n",
    "\n",
    "    recurse(doc)\n",
    "    return \" \".join(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b9fc1e",
   "metadata": {},
   "source": [
    "### Create FastText wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4697dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFastText:\n",
    "    def __init__(self, vectors, char_ngrams, vocab, size):\n",
    "        self.vectors = vectors\n",
    "        self.char_ngram_vectors = char_ngrams\n",
    "        self.vocabulary = vocab\n",
    "        self.vector_size = size\n",
    "\n",
    "    def get_vector(self, word):\n",
    "        if word in self.vectors:\n",
    "            return self.vectors[word]\n",
    "        w = f\"<{word}>\"\n",
    "        vec, count = np.zeros(self.vector_size), 0\n",
    "        for n in range(3, 7):\n",
    "            for i in range(len(w) - n + 1):\n",
    "                ngram = w[i:i+n]\n",
    "                if ngram in self.char_ngram_vectors:\n",
    "                    vec += self.char_ngram_vectors[ngram]\n",
    "                    count += 1\n",
    "        return vec / max(count, 1)\n",
    "\n",
    "    def most_similar(self, word, topn=10):\n",
    "        if word not in self.vocabulary:\n",
    "            return []\n",
    "        wv = self.get_vector(word)\n",
    "        sims = []\n",
    "        for other, vec in self.vectors.items():\n",
    "            if other == word:\n",
    "                continue\n",
    "            cos = np.dot(wv, vec) / (np.linalg.norm(wv) * np.linalg.norm(vec) + 1e-6)\n",
    "            sims.append((other, cos))\n",
    "        sims.sort(key=lambda x: x[1], reverse=True)\n",
    "        return sims[:topn]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea302156",
   "metadata": {},
   "source": [
    "### Create FastText trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab8aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_custom_fasttext(sentences, vector_size=100, window=5, min_count=5, epochs=5):\n",
    "    \"\"\"Train a simple custom FastText-like model.\"\"\"\n",
    "    print(\"\\nTraining Custom FastText Model...\")\n",
    "\n",
    "    word_counts = Counter(w for s in sentences for w in s if len(w) >= 3)\n",
    "    vocab = {w: c for w, c in word_counts.items() if c >= min_count}\n",
    "    print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "    np.random.seed(42)\n",
    "    vectors = {w: np.random.normal(0, 0.1, vector_size) for w in vocab}\n",
    "\n",
    "    char_ngram_vectors = {}\n",
    "    for word in vocab:\n",
    "        w = f\"<{word}>\"\n",
    "        for n in range(3, 7):\n",
    "            for i in range(len(w) - n + 1):\n",
    "                ngram = w[i:i+n]\n",
    "                if ngram not in char_ngram_vectors:\n",
    "                    char_ngram_vectors[ngram] = np.random.normal(0, 0.1, vector_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for sentence in tqdm(sentences, desc=f\"Training epoch {epoch+1}\"):\n",
    "            for i, target in enumerate(sentence):\n",
    "                if target not in vocab:\n",
    "                    continue\n",
    "                start = max(0, i - window)\n",
    "                end = min(len(sentence), i + window + 1)\n",
    "                for j in range(start, end):\n",
    "                    if i == j or sentence[j] not in vocab:\n",
    "                        continue\n",
    "                    context = sentence[j]\n",
    "\n",
    "                    dot = np.dot(vectors[target], vectors[context])\n",
    "                    cos_sim = dot / (np.linalg.norm(vectors[target]) * np.linalg.norm(vectors[context]) + 1e-6)\n",
    "\n",
    "                    lr = 0.01 * (1.0 / (epoch + 1))\n",
    "                    vectors[target] += lr * cos_sim * vectors[context]\n",
    "\n",
    "                    norm = np.linalg.norm(vectors[target])\n",
    "                    if norm > 0:\n",
    "                        vectors[target] /= norm\n",
    "\n",
    "    print(\"Custom FastText training done.\")\n",
    "    return CustomFastText(vectors, char_ngram_vectors, vocab, vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97fa634",
   "metadata": {},
   "source": [
    "## Develop Gensim FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "188ca8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gensim_fasttext(sentences, vector_size=100, window=5, min_count=5, epochs=5):\n",
    "    \"\"\"Train real FastText model via gensim.\"\"\"\n",
    "    print(\"\\nTraining Gensim FastText...\")\n",
    "    model = FastText(\n",
    "        sentences=sentences,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        min_n=3,\n",
    "        max_n=6,\n",
    "        epochs=epochs,\n",
    "        sg=1,\n",
    "        workers=1\n",
    "    )\n",
    "    print(\"Gensim FastText training done.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d035b3b7",
   "metadata": {},
   "source": [
    "## Compare Custom FastText with Gensim FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d566160",
   "metadata": {},
   "source": [
    "### Define comparator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1a2a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(custom_model, gensim_model):\n",
    "    print(\"\\n=== MODEL COMPARISON ===\")\n",
    "\n",
    "    custom_vocab_size = len(custom_model.vocabulary)\n",
    "    gensim_vocab_size = len(gensim_model.wv.key_to_index)\n",
    "\n",
    "    print(f\"Custom vocab: {custom_vocab_size}, Gensim vocab: {gensim_vocab_size}\")\n",
    "\n",
    "    common = set(custom_model.vocabulary.keys()) & set(gensim_model.wv.key_to_index.keys())\n",
    "    print(f\"Common words: {len(common)}\")\n",
    "\n",
    "    if common:\n",
    "        test_word = list(common)[0]\n",
    "        print(f\"\\nSimilarity test for word '{test_word}':\")\n",
    "        print(\"Custom:\", custom_model.most_similar(test_word, topn=3))\n",
    "        print(\"Gensim:\", gensim_model.wv.most_similar(test_word, topn=3))\n",
    "\n",
    "    print(\"\\nOOV test:\")\n",
    "    for w in [\"unknownword\", \"newterm123\"]:\n",
    "        try:\n",
    "            _ = custom_model.get_vector(w)\n",
    "            custom_ok = \"✓\"\n",
    "        except:\n",
    "            custom_ok = \"✗\"\n",
    "        try:\n",
    "            _ = gensim_model.wv.get_vector(w)\n",
    "            gensim_ok = \"✓\"\n",
    "        except:\n",
    "            gensim_ok = \"✗\"\n",
    "        print(f\"{w}: Custom={custom_ok}, Gensim={gensim_ok}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e75134",
   "metadata": {},
   "source": [
    "### Comparator implementer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbcf8073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(docs_dir, train_csv=None, n_docs=1000):\n",
    "    print(\"=== FASTTEXT PROGRAM ===\")\n",
    "\n",
    "    sentences = load_and_preprocess_documents(docs_dir, train_csv, n_docs)\n",
    "    if not sentences:\n",
    "        print(\"No data found.\")\n",
    "        return\n",
    "\n",
    "    start = time.time()\n",
    "    custom_model = train_custom_fasttext(sentences)\n",
    "    print(f\"Custom training took {time.time()-start:.2f} sec\")\n",
    "\n",
    "    start = time.time()\n",
    "    gensim_model = train_gensim_fasttext(sentences)\n",
    "    print(f\"Gensim training took {time.time()-start:.2f} sec\")\n",
    "\n",
    "    compare_models(custom_model, gensim_model)\n",
    "\n",
    "    print(\"\\nSaving models...\")\n",
    "    gensim_model.save(\"gensim_fasttext.model\")\n",
    "    with open(\"custom_fasttext.pkl\", \"wb\") as f:\n",
    "        pickle.dump(custom_model, f)\n",
    "    print(\"Models saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8b778b",
   "metadata": {},
   "source": [
    "## Implement comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9aedc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FASTTEXT PROGRAM ===\n",
      "Loading 1000 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading from CSV: 100%|██████████| 1000/1000 [00:01<00:00, 779.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 1000/1000 [00:03<00:00, 250.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 354736 sentences for training\n",
      "\n",
      "Training Custom FastText Model...\n",
      "Vocabulary size: 28999\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1: 100%|██████████| 354736/354736 [12:08<00:00, 487.16it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 2: 100%|██████████| 354736/354736 [11:42<00:00, 504.67it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 3: 100%|██████████| 354736/354736 [11:26<00:00, 516.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 4: 100%|██████████| 354736/354736 [11:41<00:00, 506.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 5: 100%|██████████| 354736/354736 [11:26<00:00, 516.90it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom FastText training done.\n",
      "Custom training took 3507.18 sec\n",
      "\n",
      "Training Gensim FastText...\n",
      "Gensim FastText training done.\n",
      "Gensim training took 342.77 sec\n",
      "\n",
      "=== MODEL COMPARISON ===\n",
      "Custom vocab: 28999, Gensim vocab: 28999\n",
      "Common words: 28999\n",
      "\n",
      "Similarity test for word 'barnett':\n",
      "Custom: [('experimenter', 0.48483675928085657), ('justifies', 0.469858204498151), ('muddy', 0.45716120321185527)]\n",
      "Gensim: [('bennett', 0.8931403160095215), ('barnes', 0.8920161128044128), ('barkley', 0.8908674716949463)]\n",
      "\n",
      "OOV test:\n",
      "unknownword: Custom=✓, Gensim=✓\n",
      "newterm123: Custom=✓, Gensim=✓\n",
      "\n",
      "Saving models...\n",
      "Models saved.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    docs_dir = docs_dir\n",
    "    train_csv = train_csv\n",
    "    main(docs_dir, train_csv, n_docs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e59b77",
   "metadata": {},
   "source": [
    "## Key takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15be2c45",
   "metadata": {},
   "source": [
    "The custom FastText model developed is a simple illustration of the core idea on the use of subword embeddings and handles out of vocabulary words. As expected, it performs poorly on the similarity test compared to the Gensim's FastText. Also, despite the simplicity of the custom FastText, the training took a lot longer than Gensim. To improve this model, the following can be done:\n",
    "1.  Add negative sampling: Sample random \"negative words\" that don’t occur in context and push their vectors apart.\n",
    "2. Use a better loss function: Instead of cosine similarity, implement a logistic loss (like in Word2Vec).\n",
    "3. Normalize embeddings more carefully: After each update, normalize all vectors to unit length to stabilize similarity scores.\n",
    "4. Train longer and with more data: More epochs and a bigger corpus can help, though it won’t reach Gensim-level quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
