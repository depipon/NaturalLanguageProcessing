{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02fb7778",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39523295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/davepipon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0828990e",
   "metadata": {},
   "source": [
    "## Develop Name Entity Recognition for Conditional Random Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90bc60f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRFCitationNER:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the CRF-based NER system for citation identification.\n",
    "        \"\"\"\n",
    "        self.model = sklearn_crfsuite.CRF(\n",
    "            algorithm='lbfgs',\n",
    "            c1=0.1,\n",
    "            c2=0.1,\n",
    "            max_iterations=100,\n",
    "            all_possible_transitions=True\n",
    "        )\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Common dataset indicators and patterns\n",
    "        self.dataset_indicators = {\n",
    "            'survey_terms': ['survey', 'census', 'longitudinal', 'panel', 'cohort'],\n",
    "            'data_terms': ['data', 'dataset', 'database', 'repository', 'archive'],\n",
    "            'source_terms': ['bureau', 'agency', 'administration', 'institute', 'center'],\n",
    "            'acronyms': ['CPS', 'ACS', 'NLSY', 'PSID', 'GSS', 'NHANES', 'SIPP']\n",
    "        }\n",
    "        \n",
    "        # BIO tagging scheme: B-DATASET, I-DATASET, O\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess text for better feature extraction.\n",
    "        \"\"\"\n",
    "        # Remove extra whitespace and normalize\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        return text\n",
    "    \n",
    "    def extract_word_features(self, sent, i):\n",
    "        \"\"\"\n",
    "        Extract comprehensive features for word at position i in sentence.\n",
    "        Features are designed specifically for dataset citation recognition.\n",
    "        \"\"\"\n",
    "        word = sent[i]\n",
    "        features = {\n",
    "            # Basic word features\n",
    "            'bias': 1.0,\n",
    "            'word.lower()': word.lower(),\n",
    "            'word[-3:]': word[-3:],  # suffix\n",
    "            'word[-2:]': word[-2:],  # suffix\n",
    "            'word.isupper()': word.isupper(),\n",
    "            'word.istitle()': word.istitle(),\n",
    "            'word.isdigit()': word.isdigit(),\n",
    "            'word.isalpha()': word.isalpha(),\n",
    "            'word.isalnum()': word.isalnum(),\n",
    "            'len(word)': len(word),\n",
    "            \n",
    "            # Dataset-specific features\n",
    "            'contains.digit': any(c.isdigit() for c in word),\n",
    "            'contains.hyphen': '-' in word,\n",
    "            'contains.underscore': '_' in word,\n",
    "            'contains.parentheses': '(' in word or ')' in word,\n",
    "            'all.caps': word.isupper() and len(word) > 1,\n",
    "            \n",
    "            # Acronym detection\n",
    "            'is.acronym': (word.isupper() and len(word) >= 2 and len(word) <= 6),\n",
    "            'known.dataset.acronym': word.upper() in self.dataset_indicators['acronyms'],\n",
    "            \n",
    "            # Dataset indicator terms\n",
    "            'is.survey.term': word.lower() in self.dataset_indicators['survey_terms'],\n",
    "            'is.data.term': word.lower() in self.dataset_indicators['data_terms'],\n",
    "            'is.source.term': word.lower() in self.dataset_indicators['source_terms'],\n",
    "            \n",
    "            # Punctuation features\n",
    "            'has.punctuation': any(c in string.punctuation for c in word),\n",
    "            'ends.with.period': word.endswith('.'),\n",
    "            'ends.with.comma': word.endswith(','),\n",
    "            \n",
    "            # Morphological features\n",
    "            'stemmed': self.stemmer.stem(word.lower()),\n",
    "            'lemmatized': self.lemmatizer.lemmatize(word.lower()),\n",
    "        }\n",
    "        \n",
    "        # Context features (previous and next words)\n",
    "        if i > 0:\n",
    "            word1 = sent[i-1]\n",
    "            features.update({\n",
    "                '-1:word.lower()': word1.lower(),\n",
    "                '-1:word.istitle()': word1.istitle(),\n",
    "                '-1:word.isupper()': word1.isupper(),\n",
    "                '-1:is.survey.term': word1.lower() in self.dataset_indicators['survey_terms'],\n",
    "                '-1:is.data.term': word1.lower() in self.dataset_indicators['data_terms'],\n",
    "                '-1:is.source.term': word1.lower() in self.dataset_indicators['source_terms'],\n",
    "            })\n",
    "        else:\n",
    "            features['BOS'] = True  # Beginning of sentence\n",
    "            \n",
    "        if i < len(sent)-1:\n",
    "            word1 = sent[i+1]\n",
    "            features.update({\n",
    "                '+1:word.lower()': word1.lower(),\n",
    "                '+1:word.istitle()': word1.istitle(),\n",
    "                '+1:word.isupper()': word1.isupper(),\n",
    "                '+1:is.survey.term': word1.lower() in self.dataset_indicators['survey_terms'],\n",
    "                '+1:is.data.term': word1.lower() in self.dataset_indicators['data_terms'],\n",
    "                '+1:is.source.term': word1.lower() in self.dataset_indicators['source_terms'],\n",
    "            })\n",
    "        else:\n",
    "            features['EOS'] = True  # End of sentence\n",
    "            \n",
    "        # Bigram features\n",
    "        if i > 0:\n",
    "            features['-1,0:words'] = f\"{sent[i-1].lower()}_{word.lower()}\"\n",
    "        if i < len(sent)-1:\n",
    "            features['0,+1:words'] = f\"{word.lower()}_{sent[i+1].lower()}\"\n",
    "            \n",
    "        # Window features (Â±2 words)\n",
    "        if i > 1:\n",
    "            features['-2:word.lower()'] = sent[i-2].lower()\n",
    "        if i < len(sent)-2:\n",
    "            features['+2:word.lower()'] = sent[i+2].lower()\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def sent2features(self, sent):\n",
    "        \"\"\"Convert sentence to feature vectors.\"\"\"\n",
    "        return [self.extract_word_features(sent, i) for i in range(len(sent))]\n",
    "    \n",
    "    def sent2labels(self, sent_labels):\n",
    "        \"\"\"Extract labels from sentence.\"\"\"\n",
    "        return sent_labels\n",
    "    \n",
    "    def sent2tokens(self, sent):\n",
    "        \"\"\"Extract tokens from sentence.\"\"\"\n",
    "        return sent\n",
    "    \n",
    "    def create_training_data_from_text(self, text_samples, annotations):\n",
    "        \"\"\"\n",
    "        Create training data from text samples and their annotations.\n",
    "        \n",
    "        Args:\n",
    "            text_samples: List of text strings\n",
    "            annotations: List of annotations in format [{'start': int, 'end': int, 'label': str}]\n",
    "        \n",
    "        Returns:\n",
    "            X_train, y_train: Features and labels for training\n",
    "        \"\"\"\n",
    "        X, y = [], []\n",
    "        \n",
    "        for text, annots in zip(text_samples, annotations):\n",
    "            # Tokenize text\n",
    "            tokens = word_tokenize(text)\n",
    "            labels = ['O'] * len(tokens)\n",
    "            \n",
    "            # Convert character-based annotations to token-based labels\n",
    "            char_to_token = self._create_char_to_token_mapping(text, tokens)\n",
    "            \n",
    "            for annot in annots:\n",
    "                start_char, end_char = annot['start'], annot['end']\n",
    "                entity_type = annot.get('label', 'DATASET')\n",
    "                \n",
    "                # Find corresponding tokens\n",
    "                start_token = char_to_token.get(start_char)\n",
    "                end_token = char_to_token.get(end_char - 1)  # end_char is exclusive\n",
    "                \n",
    "                if start_token is not None and end_token is not None:\n",
    "                    # Apply BIO tagging\n",
    "                    labels[start_token] = f'B-{entity_type}'\n",
    "                    for i in range(start_token + 1, end_token + 1):\n",
    "                        if i < len(labels):\n",
    "                            labels[i] = f'I-{entity_type}'\n",
    "            \n",
    "            X.append(tokens)\n",
    "            y.append(labels)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def _create_char_to_token_mapping(self, text, tokens):\n",
    "        \"\"\"Create mapping from character positions to token indices.\"\"\"\n",
    "        char_to_token = {}\n",
    "        char_idx = 0\n",
    "        \n",
    "        for token_idx, token in enumerate(tokens):\n",
    "            # Find token in text starting from current position\n",
    "            while char_idx < len(text) and text[char_idx].isspace():\n",
    "                char_idx += 1\n",
    "            \n",
    "            # Map all characters in this token\n",
    "            token_start = char_idx\n",
    "            for i in range(len(token)):\n",
    "                if char_idx < len(text):\n",
    "                    char_to_token[char_idx] = token_idx\n",
    "                    char_idx += 1\n",
    "        \n",
    "        return char_to_token\n",
    "    \n",
    "    def prepare_training_data(self, X_tokens, y_labels):\n",
    "        \"\"\"\n",
    "        Prepare feature vectors and labels for CRF training.\n",
    "        \"\"\"\n",
    "        X_features = [self.sent2features(sent) for sent in X_tokens]\n",
    "        y_formatted = [self.sent2labels(labels) for labels in y_labels]\n",
    "        \n",
    "        return X_features, y_formatted\n",
    "    \n",
    "    def train(self, X_tokens, y_labels):\n",
    "        \"\"\"\n",
    "        Train the CRF model.\n",
    "        \n",
    "        Args:\n",
    "            X_tokens: List of tokenized sentences\n",
    "            y_labels: List of label sequences (BIO format)\n",
    "        \"\"\"\n",
    "        print(\"Preparing training data...\")\n",
    "        X_train, y_train = self.prepare_training_data(X_tokens, y_labels)\n",
    "        \n",
    "        print(\"Training CRF model...\")\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_tokens):\n",
    "        \"\"\"\n",
    "        Predict labels for tokenized sentences.\n",
    "        \"\"\"\n",
    "        X_features = [self.sent2features(sent) for sent in X_tokens]\n",
    "        return self.model.predict(X_features)\n",
    "    \n",
    "    def predict_text(self, text):\n",
    "        \"\"\"\n",
    "        Predict entities in raw text.\n",
    "        \n",
    "        Returns:\n",
    "            List of dictionaries with 'text', 'label', 'start', 'end'\n",
    "        \"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        entities = []\n",
    "        char_offset = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            # Find sentence position in original text\n",
    "            sent_start = text.find(sentence, char_offset)\n",
    "            if sent_start == -1:\n",
    "                sent_start = char_offset\n",
    "                \n",
    "            tokens = word_tokenize(sentence)\n",
    "            if not tokens:\n",
    "                char_offset = sent_start + len(sentence)\n",
    "                continue\n",
    "                \n",
    "            predictions = self.predict([tokens])[0]\n",
    "            \n",
    "            # Convert predictions back to entities\n",
    "            current_entity = None\n",
    "            token_start = sent_start\n",
    "            \n",
    "            for token, label in zip(tokens, predictions):\n",
    "                # Find token position in sentence\n",
    "                token_pos = sentence.find(token, token_start - sent_start)\n",
    "                if token_pos != -1:\n",
    "                    token_abs_start = sent_start + token_pos\n",
    "                    token_abs_end = token_abs_start + len(token)\n",
    "                else:\n",
    "                    # Fallback: approximate position\n",
    "                    token_abs_start = token_start\n",
    "                    token_abs_end = token_start + len(token)\n",
    "                \n",
    "                if label.startswith('B-'):\n",
    "                    # Start of new entity\n",
    "                    if current_entity:\n",
    "                        entities.append(current_entity)\n",
    "                    current_entity = {\n",
    "                        'text': token,\n",
    "                        'label': label[2:],\n",
    "                        'start': token_abs_start,\n",
    "                        'end': token_abs_end\n",
    "                    }\n",
    "                elif label.startswith('I-') and current_entity:\n",
    "                    # Continue current entity\n",
    "                    current_entity['text'] += f\" {token}\"\n",
    "                    current_entity['end'] = token_abs_end\n",
    "                else:\n",
    "                    # End current entity\n",
    "                    if current_entity:\n",
    "                        entities.append(current_entity)\n",
    "                        current_entity = None\n",
    "                \n",
    "                token_start = token_abs_end\n",
    "            \n",
    "            # Don't forget the last entity\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "            \n",
    "            char_offset = sent_start + len(sentence)\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def evaluate(self, X_test_tokens, y_test_labels):\n",
    "        \"\"\"\n",
    "        Evaluate the model performance.\n",
    "        \"\"\"\n",
    "        X_test, y_test = self.prepare_training_data(X_test_tokens, y_test_labels)\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        \n",
    "        # Flatten for evaluation\n",
    "        y_true_flat = [label for sent_labels in y_test for label in sent_labels]\n",
    "        y_pred_flat = [label for sent_labels in y_pred for label in sent_labels]\n",
    "        \n",
    "        # Print detailed metrics\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_true_flat, y_pred_flat))\n",
    "        \n",
    "        # Entity-level metrics\n",
    "        print(\"\\nEntity-level F1 scores:\")\n",
    "        labels = list(self.model.classes_)\n",
    "        for label in labels:\n",
    "            if label != 'O':\n",
    "                f1 = f1_score(y_true_flat, y_pred_flat, labels=[label], average='micro')\n",
    "                print(f\"{label}: {f1:.4f}\")\n",
    "        \n",
    "        # Overall performance\n",
    "        overall_f1 = f1_score(y_true_flat, y_pred_flat, average='weighted')\n",
    "        print(f\"\\nOverall weighted F1: {overall_f1:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'classification_report': classification_report(y_true_flat, y_pred_flat, output_dict=True),\n",
    "            'overall_f1': overall_f1,\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the trained model.\"\"\"\n",
    "        model_data = {\n",
    "            'model': self.model,\n",
    "            'dataset_indicators': self.dataset_indicators,\n",
    "        }\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load a trained model.\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        self.model = model_data['model']\n",
    "        self.dataset_indicators = model_data['dataset_indicators']\n",
    "        print(f\"Model loaded from {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c43e521",
   "metadata": {},
   "source": [
    "## Randomly sample trianing and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a966308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly sampled 1000 annotated texts.\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "train_csv = \"/Users/davepipon/Desktop/DS397 Data/coleridgeinitiative-show-us-the-data/train.csv\"\n",
    "docs_dir = \"/Users/davepipon/Desktop/DS397 Data/coleridgeinitiative-show-us-the-data/train/\"\n",
    "\n",
    "df = pd.read_csv(train_csv)\n",
    "\n",
    "# Shuffle rows to ensure randomness\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "text_samples = []\n",
    "annotations = []\n",
    "\n",
    "# Collect annotated samples\n",
    "for _, row in df.iterrows():\n",
    "    doc_id = row[\"Id\"]\n",
    "    dataset_label = row[\"dataset_label\"]\n",
    "\n",
    "    doc_path = os.path.join(docs_dir, f\"{doc_id}.json\")\n",
    "    if not os.path.exists(doc_path):\n",
    "        continue\n",
    "\n",
    "    with open(doc_path, \"r\") as f:\n",
    "        doc_json = json.load(f)\n",
    "\n",
    "    for section in doc_json:\n",
    "        text = section.get(\"text\", \"\")\n",
    "        if dataset_label.lower() in text.lower():\n",
    "            # record sample\n",
    "            text_samples.append(text)\n",
    "\n",
    "            # find all mentions of dataset_label\n",
    "            annots = []\n",
    "            start = 0\n",
    "            while True:\n",
    "                idx = text.lower().find(dataset_label.lower(), start)\n",
    "                if idx == -1:\n",
    "                    break\n",
    "                annots.append({\n",
    "                    \"start\": idx,\n",
    "                    \"end\": idx + len(dataset_label),\n",
    "                    \"label\": \"DATASET\"\n",
    "                })\n",
    "                start = idx + len(dataset_label)\n",
    "\n",
    "            annotations.append(annots)\n",
    "\n",
    "# Randomly select 1000 samples\n",
    "sample_indices = random.sample(range(len(text_samples)), min(1000, len(text_samples)))\n",
    "text_samples = [text_samples[i] for i in sample_indices]\n",
    "annotations = [annotations[i] for i in sample_indices]\n",
    "\n",
    "print(f\"Randomly sampled {len(text_samples)} annotated texts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a819142",
   "metadata": {},
   "source": [
    "## Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f81b0565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "Training CRF model...\n",
      "Training completed!\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-DATASET       0.77      0.76      0.77       375\n",
      "   I-DATASET       0.76      0.71      0.73       469\n",
      "           O       1.00      1.00      1.00    123005\n",
      "\n",
      "    accuracy                           1.00    123849\n",
      "   macro avg       0.84      0.82      0.83    123849\n",
      "weighted avg       1.00      1.00      1.00    123849\n",
      "\n",
      "\n",
      "Entity-level F1 scores:\n",
      "B-DATASET: 0.7668\n",
      "I-DATASET: 0.7341\n",
      "\n",
      "Overall weighted F1: 0.9969\n"
     ]
    }
   ],
   "source": [
    "# Initialize CRF model\n",
    "ner = CRFCitationNER()\n",
    "\n",
    "# Convert texts + annotations â token/label sequences\n",
    "X_tokens, y_labels = ner.create_training_data_from_text(text_samples, annotations)\n",
    "\n",
    "# Train/test split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tokens, y_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "ner.train(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "results = ner.evaluate(X_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
