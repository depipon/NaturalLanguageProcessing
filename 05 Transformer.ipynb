{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47851042",
   "metadata": {},
   "source": [
    "# Develop transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90a2552",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0385d06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x142d90530>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "PreTrainedTokenizerFast,\n",
    "GPT2Config,\n",
    "GPT2LMHeadModel,\n",
    "Trainer,\n",
    "TrainingArguments,\n",
    "pipeline,\n",
    ")\n",
    "\n",
    "import os\n",
    "os.environ[\"DISABLE_MLFLOW_INTEGRATION\"] = \"TRUE\"\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e09751",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c2f87221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sampled JSON files: 100\n",
      "Snippet of first doc:\n",
      " alzheimer's disease and other types of dementia are the top cause for disabilities in later life and various types of experiments have been performed to understand the underlying mechanisms of the disease with the aim of coming up with potential drug targets. these experiments have been carried out  ...\n"
     ]
    }
   ],
   "source": [
    "# Set the path to your dataset directory\n",
    "path = os.path.expanduser(\"/Users/davepipon/Desktop/DS397 Data/coleridgeinitiative-show-us-the-data/train/*.json\")\n",
    "\n",
    "# Load training file names\n",
    "files = glob.glob(path)\n",
    "\n",
    "# Randomly select 100 files\n",
    "sample_files = random.sample(files, min(100, len(files)))\n",
    "\n",
    "# Read and store randomly sampled documents\n",
    "sample_data = []\n",
    "for path in sample_files:\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "        for entry in data:\n",
    "            sample_data.append(entry.get(\"text\", \"\"))\n",
    "\n",
    "print(\"Number of sampled JSON files:\", len(sample_files))\n",
    "print(\"Snippet of first doc:\\n\", sample_data[0][:300], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccbbc61",
   "metadata": {},
   "source": [
    "### Set up training and heldout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "77fedb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training documents: 1862\n",
      "Number of heldout documents: 207\n",
      "Snippet of first training doc:\n",
      " In vitro studies have revealed that SARS-CoV-2 can infect human lung tissue more effectively and replicate more efficiently compared with SARS-CoV. The number of viral particles in lung tissues infected by SARS-CoV-2 is more than 3.2 times the number of SARS-CoV within 48 h. [82] Blocking viral repl ...\n",
      "Snippet of first heldout doc:\n",
      " The NIH Alzheimer's Disease Neuroimaging Initiative (ADNI) [15] is an ongoing five-year public-private partnership to test whether serial MRI, PET, SNP, other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (M ...\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(sample_data)\n",
    "\n",
    "train_ratio = 0.9\n",
    "split_idx = int(len(sample_data) * train_ratio)\n",
    "\n",
    "train_docs = sample_data[:split_idx]\n",
    "heldout_docs = sample_data[split_idx:]\n",
    "\n",
    "# Check snippet of train and heldout docs\n",
    "print(\"Number of training documents:\", len(train_docs))\n",
    "print(\"Number of heldout documents:\", len(heldout_docs))\n",
    "print(\"Snippet of first training doc:\\n\", train_docs[0][:300], \"...\")\n",
    "print(\"Snippet of first heldout doc:\\n\", heldout_docs[0][:300], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf650074",
   "metadata": {},
   "source": [
    "## Develop algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeb2073",
   "metadata": {},
   "source": [
    "### Train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa5be51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('wordpiece_tokenizer/tokenizer_config.json',\n",
       " 'wordpiece_tokenizer/special_tokens_map.json',\n",
       " 'wordpiece_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Develop Wordpiece tokenizer\n",
    "def train_tokenizer(documents, vocab_size=8000):\n",
    "\ttokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\ttokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "\ttrainer = WordPieceTrainer(\n",
    "\t\tvocab_size=vocab_size,\n",
    "\t\tspecial_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
    "\t)\n",
    "\n",
    "\ttokenizer.train_from_iterator(documents, trainer)\n",
    "\ttokenizer.save(\"tokenizer.json\")\n",
    "\treturn tokenizer\n",
    "\n",
    "\n",
    "raw_tokenizer = train_tokenizer(train_docs)\n",
    "\n",
    "block_size = 128\n",
    "data = {\n",
    "\t\"input_ids\": [],\n",
    "\t\"labels\": []\n",
    "}\n",
    "\n",
    "for doc in train_docs:\n",
    "\ttokens = raw_tokenizer.encode(doc).ids\n",
    "\tfor i in range(0, len(tokens) - block_size, block_size):\n",
    "\t\tinput_ids = tokens[i:i+block_size]\n",
    "\t\tlabels = tokens[i+1:i+block_size+1]\n",
    "\t\tdata[\"input_ids\"].append(input_ids)\n",
    "\t\tdata[\"labels\"].append(labels)\n",
    "\n",
    "hf_tokenizer = PreTrainedTokenizerFast(\n",
    "\ttokenizer_file=\"tokenizer.json\",\n",
    "\tpad_token=\"[PAD]\",\n",
    "\tunk_token=\"[UNK]\",\n",
    "\tbos_token=\"[BOS]\",\n",
    "\teos_token=\"[EOS]\",\n",
    ")\n",
    "\n",
    "hf_tokenizer.save_pretrained(\"wordpiece_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a154e29",
   "metadata": {},
   "source": [
    "### Tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243cab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reclass data into PyTorch Dataset\n",
    "class LMDataset(Dataset):\n",
    "\tdef __init__(self, data):\n",
    "\t\tself.input_ids = data[\"input_ids\"]\n",
    "\t\tself.labels = data[\"labels\"]\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.input_ids)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn {\n",
    "\t\t\t\"input_ids\": torch.tensor(self.input_ids[idx]),\n",
    "\t\t\t\"labels\": torch.tensor(self.labels[idx]),\n",
    "\t\t}\n",
    "\n",
    "\n",
    "train_dataset = LMDataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3292ea04",
   "metadata": {},
   "source": [
    "### Define decoder-only transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5facd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up GPT-2 configuration and model\n",
    "config = GPT2Config(\n",
    "vocab_size=hf_tokenizer.vocab_size,\n",
    "n_positions=block_size,\n",
    "n_embd=256,\n",
    "n_layer=4,\n",
    "n_head=4,\n",
    "bos_token_id=hf_tokenizer.bos_token_id,\n",
    "eos_token_id=hf_tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c686074",
   "metadata": {},
   "source": [
    "### Set up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d174dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tz/gs_7p8rj7tz_vx4snsdf_c2m0000gp/T/ipykernel_3233/262587271.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 04:23, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>7.593300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>7.186900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>7.090100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>7.095500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>7.062400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>7.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>7.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>6.994600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>6.932800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.920900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>6.898200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>6.893400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>6.859100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>6.811800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>6.823500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>6.803800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>6.791000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>6.779400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>6.777300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>6.799900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('trained_gpt/tokenizer_config.json',\n",
       " 'trained_gpt/special_tokens_map.json',\n",
       " 'trained_gpt/tokenizer.json')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Develop training arguments and Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./trained_gpt\",\n",
    "    overwrite_output_dir=True,\n",
    "    max_steps=1000,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_steps=400,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "model=model,\n",
    "args=training_args,\n",
    "train_dataset=train_dataset,\n",
    "tokenizer=hf_tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"trained_gpt\")\n",
    "hf_tokenizer.save_pretrained(\"trained_gpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1605740",
   "metadata": {},
   "source": [
    "## Generate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e23b9c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data from the study were collected is the of of of of the was - of of.,,, a - of - -., percent ( )., - ) the of,,,, of the of. the of the and -, was of the of and. percent in -,,, to - -. the of. the to is -, in of.., is that to, of is that\n"
     ]
    }
   ],
   "source": [
    "# Text generation using the trained model\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"./trained_gpt\",\n",
    "    tokenizer=\"./trained_gpt\",\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    ")\n",
    "\n",
    "prompt = \"data from the study were collected\"\n",
    "\n",
    "outputs = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=80,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    pad_token_id=hf_tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb8f8b",
   "metadata": {},
   "source": [
    "## Calculate perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc0f54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Held-out perplexity: 1242.55\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity on held-out documents\n",
    "def transformer_perplexity(model, tokenizer, texts):\n",
    "\tmodel.eval()\n",
    "\ttotal_loss = 0.0\n",
    "\ttotal_tokens = 0\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor text in texts:\n",
    "\t\t\tif len(text) == 0:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tenc = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=block_size, padding=True)\n",
    "\t\t\toutputs = model(**enc, labels=enc[\"input_ids\"])\n",
    "\t\t\tloss = outputs.loss\n",
    "\t\t\ttotal_loss += loss.item() * enc[\"input_ids\"].size(1)\n",
    "\t\t\ttotal_tokens += enc[\"input_ids\"].size(1)\n",
    "\n",
    "\tif total_tokens == 0:\n",
    "\t\treturn float('inf')\n",
    "\treturn math.exp(total_loss / total_tokens)\n",
    "\n",
    "\n",
    "pp = transformer_perplexity(model, hf_tokenizer, heldout_docs)\n",
    "print(f\"Held-out perplexity: {pp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da1c5f1",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e139ee7",
   "metadata": {},
   "source": [
    "Even when trained on only 100 JSON files, the transformer-based decoder model trained faster than the trigram model. Although trigram models are simple per estimate, they require extensive counting and smoothing, which increases preprocessing and memory overhead. In contrast, the transformer benefits from mini-batch optimization and efficient matrix operations.\n",
    "\n",
    "Predictively, the transformer achieved roughly half the perplexity of the trigram model, reflecting a better fit to the data. Its ability to capture long-range context and learn distributed token representations accounts for this improvement. These results highlight the superior efficiency and modeling capacity of neural language models over n-grams."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
