{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7259f6",
   "metadata": {},
   "source": [
    "# Bottom-up Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f416cb",
   "metadata": {},
   "source": [
    "## Develop an algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a50749c",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d005b55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/davepipon/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, wordpunct_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "import os, json, glob\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34e9a37",
   "metadata": {},
   "source": [
    "### Load sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1a83270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sampled JSON files: 2000\n",
      "Number of texts stored: 37728\n",
      "Snippet of first doc:\n",
      " alzheimer's disease and other types of dementia are the top cause for disabilities in later life and various types of experiments have been performed to understand the underlying mechanisms of the disease with the aim of coming up with potential drug targets. these experiments have been carried out  ...\n"
     ]
    }
   ],
   "source": [
    "# Set the path to your dataset directory\n",
    "path = os.path.expanduser(\"/Users/davepipon/Desktop/DS397 Data/coleridgeinitiative-show-us-the-data/train/*.json\")\n",
    "\n",
    "# Load training file names\n",
    "files = glob.glob(path)\n",
    "\n",
    "# Randomly select 2000 files\n",
    "sample_files = random.sample(files, min(2000, len(files)))\n",
    "\n",
    "# Read and store randomly sampled documents\n",
    "sample_data = []\n",
    "for path in sample_files:\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "        for entry in data:\n",
    "            sample_data.append(entry.get(\"text\", \"\"))\n",
    "\n",
    "print(\"Number of sampled JSON files:\", len(sample_files))\n",
    "print(\"Number of texts stored:\", len(sample_data))\n",
    "print(\"Snippet of first doc:\\n\", sample_data[0][:300], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cef3eec",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab220c9b",
   "metadata": {},
   "source": [
    "#### Generate corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05c2bc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 115330\n",
      "Top 10 tokens by frequency: [('the', 863626), ('of', 503701), ('and', 454964), ('in', 362744), ('to', 316623), ('a', 234964), ('for', 172715), ('is', 135841), ('that', 131452), ('with', 113159)]\n"
     ]
    }
   ],
   "source": [
    "# Simple regex tokenizer (split on non-alphabetic chars)\n",
    "tokens = []\n",
    "for doc in sample_data:\n",
    "    words = re.findall(r\"\\b\\w+\\b\", doc.lower())  # lowercase for consistency\n",
    "    tokens.extend(words)\n",
    "\n",
    "# Remove tokens that has special characters or numbers\n",
    "tokens = re.findall(r'[a-zA-Z]+', ' '.join(tokens))\n",
    "\n",
    "# Generate unique tokens with frequency\n",
    "token_freq = Counter(tokens)\n",
    "\n",
    "# Sort tokens by frequency\n",
    "corpus = sorted(token_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Number of unique tokens:\", len(token_freq))\n",
    "print(\"Top 10 tokens by frequency:\", corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e6126b",
   "metadata": {},
   "source": [
    "#### Create initial vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e01d9909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocab size: 26\n",
      "Initial vocab: ['t', 'o', 'h', 'z', 'k', 'l', 'p', 'y', 'j', 'b', 'q', 'r', 'u', 'x', 's', 'a', 'c', 'w', 'g', 'f', 'e', 'd', 'v', 'i', 'm', 'n']\n"
     ]
    }
   ],
   "source": [
    "# Extract unique list of letters from corpus as initial vocab\n",
    "unique_letters = re.sub(r'[^a-zA-Z\\s]', '', ''.join(tokens))\n",
    "vocab = list(set(unique_letters))\n",
    "\n",
    "print(\"Initial vocab size:\", len(vocab))\n",
    "print(\"Initial vocab:\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33c01e9",
   "metadata": {},
   "source": [
    "### Implement bottom-up tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad3c771",
   "metadata": {},
   "source": [
    "#### Byte-pairing encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e26bccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to count frequency of adjacent symbol pairs\n",
    "def get_pair_stats(corpus):\n",
    "    pair_freq = defaultdict(int)\n",
    "    for symbols, freq in corpus:\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pair = (symbols[i], symbols[i + 1])\n",
    "            pair_freq[pair] += freq\n",
    "    return pair_freq\n",
    "\n",
    "# Define function to merge the most frequent pair in the corpus\n",
    "def merge_pair(pair, corpus):\n",
    "    a, b = pair\n",
    "    new_corpus = []\n",
    "    for symbols, freq in corpus:\n",
    "        new_syms = []\n",
    "        i = 0\n",
    "        while i < len(symbols):\n",
    "            if i < len(symbols)-1 and symbols[i] == a and symbols[i+1] == b:\n",
    "                new_syms.append(a+b)   # merge\n",
    "                i += 2\n",
    "            else:\n",
    "                new_syms.append(symbols[i])\n",
    "                i += 1\n",
    "        new_corpus.append((new_syms, freq))\n",
    "    return new_corpus\n",
    "\n",
    "# Main function to perform byte-pair encoding\n",
    "def byte_pair_encoding(corpus, vocab, num_merges=100):\n",
    "    \"\"\"\n",
    "    corpus: list of (word, freq) where word is a string.\n",
    "    vocab: initial vocab.\n",
    "    \"\"\"\n",
    "    # initialize corpus as list of (symbols, freq)\n",
    "    corpus = [(list(word), freq) for word, freq in corpus]\n",
    "\n",
    "    for _ in range(num_merges):\n",
    "        pair_freq = get_pair_stats(corpus)\n",
    "        if not pair_freq:\n",
    "            break\n",
    "        best_pair = max(pair_freq, key=pair_freq.get)   # most frequent\n",
    "        vocab.append(''.join(best_pair))                # add to vocab\n",
    "        corpus = merge_pair(best_pair, corpus)          # update corpus\n",
    "\n",
    "    return vocab, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d64f7bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE vocab size: 40026\n",
      "BPE vocab snippet: ['t', 'o', 'h', 'z', 'k', 'l', 'p', 'y', 'j', 'b', 'q', 'r', 'u', 'x', 's', 'a', 'c', 'w', 'g', 'f', 'e', 'd', 'v', 'i', 'm', 'n', 'th', 'in', 're', 'the', 'on', 'at', 'an', 'er', 'en', 'al', 'st', 'or', 'ed', 'es', 'ion', 'of', 'and', 'ar', 'as', 'ic', 'it', 'ro', 'ing', 'is']\n",
      "['fulfils', 'canned', 'holmst', 'holmstrom', 'mukhopadhaya', 'royer', 'samuelson', 'pursues', 'atraumatic', 'endeavours', 'unveiled', 'electra', 'subtitle', 'pointers', 'tragic', 'reallocate', 'straightforwardly', 'cranfield', 'cyril', 'negligibly', 'reusability', 'rbp', 'showcase', 'wordpiece', 'inflections', 'tricks', 'idiosyncras', 'idiosyncrasies', 'finetuning', 'segregating', 'annotating', 'distilling', 'sackett', 'ordinances', 'shorthand', 'derog', 'derogatory', 'learnings', 'giants', 'hepes', 'deionized', 'calmodulin', 'justin', 'casulli', 'whitecapping', 'morison', 'prestia', 'lipscomb', 'unclustered', 'garces']\n"
     ]
    }
   ],
   "source": [
    "# Implement BPE\n",
    "num_merges = 40000\n",
    "expanded_vocab, final_corpus = byte_pair_encoding(corpus, vocab, num_merges=num_merges)\n",
    "print(\"BPE vocab size:\", len(expanded_vocab))\n",
    "print(\"BPE vocab snippet:\", expanded_vocab[:50])\n",
    "print(expanded_vocab[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fe73b5",
   "metadata": {},
   "source": [
    "# Compare with HuggingFace WordPiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa9fb1a",
   "metadata": {},
   "source": [
    "## Tokenization using WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4eee7c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "WordPiece vocab size: 40000\n",
      "WordPiece vocab snippet: [('arsen', 30748), ('stuart', 23765), ('fundamental', 5632), ('lsu', 36341), ('malta', 26586), ('compu', 2365), ('demonstrates', 6189), ('conclusions', 3657), ('structurally', 18853), ('paired', 7495), ('systolic', 9438), ('mumford', 34169), ('pressed', 33732), ('produc', 548), ('progra', 555), ('affection', 37267), ('gaining', 10239), ('detritus', 39497), ('##essional', 27467), ('save', 11113), ('waynes', 33731), ('cau', 5505), ('bailey', 17657), ('uncon', 7915), ('listing', 7232), ('kriging', 16353), ('##well', 6996), ('natic', 26838), ('##ept', 31600), ('##iology', 6378), ('##ulation', 589), ('tedesco', 29686), ('##be', 2450), ('##umines', 31520), ('variates', 35800), ('##gue', 18802), ('muff', 22212), ('cofound', 36945), ('ncl', 9785), ('carbonaceous', 39113), ('##yder', 16842), ('kat', 6740), ('norovirus', 30783), ('harvey', 14702), ('##as', 81), ('facs', 34755), ('suspension', 12481), ('seldom', 15014), ('##isen', 15935), ('keyp', 34794)]\n",
      "[('exp', 243), ('kar', 4472), ('##ishments', 28662), ('##lchol', 20024), ('##ulus', 5354), ('untargeted', 36394), ('##ibrillary', 8368), ('hydrographic', 8121), ('##olachlor', 21239), ('##aline', 20999), ('starts', 9279), ('between', 298), ('cz', 10252), ('appadurai', 28038), ('environments', 4203), ('worrying', 37882), ('ist', 18622), ('##mil', 34440), ('##lands', 4682), ('exciting', 15774), ('fetus', 35246), ('cardoso', 37680), ('gd', 11136), ('harmonize', 22499), ('tja', 38348), ('achieves', 11538), ('nearing', 38905), ('provoke', 27653), ('plat', 3826), ('##view', 773), ('beneficiary', 31142), ('conceptualization', 15662), ('coupled', 5229), ('maturity', 16735), ('physician', 8503), ('formulated', 9616), ('epidemiologically', 35341), ('photographed', 29685), ('musculoskeletal', 35488), ('##veen', 38496), ('beverages', 13034), ('##lov', 38675), ('serotonin', 17576), ('ppe', 20177), ('decadal', 7310), ('csak', 29496), ('eu', 2026), ('src', 37598), ('fracturing', 35170), ('upon', 2950)]\n"
     ]
    }
   ],
   "source": [
    "# Define a generator to yield single-token sentences for WordPiece tokenizer\n",
    "def corpus_iterator(corpus):\n",
    "    for word, freq in corpus:\n",
    "        for _ in range(freq):\n",
    "            yield [word]  # single-token sentence\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=True)\n",
    "tokenizer.train_from_iterator(\n",
    "    corpus_iterator(corpus),\n",
    "    vocab_size=40000,  # Set desired vocab size\n",
    "    min_frequency=1,  # Minimum frequency for a token to be included\n",
    ")\n",
    "\n",
    "print(\"WordPiece vocab size:\", tokenizer.get_vocab_size())\n",
    "print(\"WordPiece vocab snippet:\", list(tokenizer.get_vocab().items())[:50])\n",
    "print(list(tokenizer.get_vocab().items())[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7acaa93",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0948ab06",
   "metadata": {},
   "source": [
    "When comparing the developed BPE and WordPiece tokenizers, the difference in token size mainly depends on their respective hyperparameters. Specifically, the BPE tokenizer’s vocabulary size is controlled by the number of merge operations ***k***, while the WordPiece tokenizer is directly constrained by the predefined vocabulary size. A key distinction is that BPE’s learned tokens are influenced by its initial vocabulary, whereas WordPiece explicitly introduces the prefix “##” to mark subword continuations. Importantly, sufficient training iterations are necessary to ensure that frequent English words are adequately represented in the vocabulary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
