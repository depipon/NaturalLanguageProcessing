{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7259f6",
   "metadata": {},
   "source": [
    "# Bottom-up Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f416cb",
   "metadata": {},
   "source": [
    "## Develop an algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a50749c",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d005b55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/davepipon/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, wordpunct_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "import os, json, glob\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34e9a37",
   "metadata": {},
   "source": [
    "### Load sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1a83270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sampled JSON files: 2000\n",
      "Number of texts stored: 37728\n",
      "Snippet of first doc:\n",
      " alzheimer's disease and other types of dementia are the top cause for disabilities in later life and various types of experiments have been performed to understand the underlying mechanisms of the disease with the aim of coming up with potential drug targets. these experiments have been carried out  ...\n"
     ]
    }
   ],
   "source": [
    "# Set the path to your dataset directory\n",
    "path = os.path.expanduser(\"/Users/davepipon/Desktop/DS397 Data/coleridgeinitiative-show-us-the-data/train/*.json\")\n",
    "\n",
    "# Load training file names\n",
    "files = glob.glob(path)\n",
    "\n",
    "# Randomly select 2000 files\n",
    "sample_files = random.sample(files, min(2000, len(files)))\n",
    "\n",
    "# Read and store randomly sampled documents\n",
    "sample_data = []\n",
    "for path in sample_files:\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "        for entry in data:\n",
    "            sample_data.append(entry.get(\"text\", \"\"))\n",
    "\n",
    "print(\"Number of sampled JSON files:\", len(sample_files))\n",
    "print(\"Number of texts stored:\", len(sample_data))\n",
    "print(\"Snippet of first doc:\\n\", sample_data[0][:300], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cef3eec",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95428e1",
   "metadata": {},
   "source": [
    "#### Segment to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ad28081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 599606\n"
     ]
    }
   ],
   "source": [
    "# Segment to sentences\n",
    "all_sentences = []\n",
    "for doc in sample_data:\n",
    "    sents = sent_tokenize(doc)\n",
    "    all_sentences.extend(sents)\n",
    "\n",
    "# Ensure there are no empty sentences\n",
    "all_sentences = [sent for sent in all_sentences if len(sent) > 0]\n",
    "\n",
    "print(\"Number of sentences:\", len(all_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab220c9b",
   "metadata": {},
   "source": [
    "#### Generate corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05c2bc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 115330\n",
      "Top 10 tokens by frequency: [('the', 863626), ('of', 503701), ('and', 454964), ('in', 362744), ('to', 316623), ('a', 234964), ('for', 172715), ('is', 135841), ('that', 131452), ('with', 113159)]\n"
     ]
    }
   ],
   "source": [
    "# Simple regex tokenizer (split on non-alphabetic chars)\n",
    "tokens = []\n",
    "for doc in sample_data:\n",
    "    words = re.findall(r\"\\b\\w+\\b\", doc.lower())  # lowercase for consistency\n",
    "    tokens.extend(words)\n",
    "\n",
    "# Remove tokens that has special characters or numbers\n",
    "tokens = re.findall(r'[a-zA-Z]+', ' '.join(tokens))\n",
    "\n",
    "# Generate unique tokens with frequency\n",
    "token_freq = Counter(tokens)\n",
    "\n",
    "# Sort tokens by frequency\n",
    "corpus = sorted(token_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Number of unique tokens:\", len(token_freq))\n",
    "print(\"Top 10 tokens by frequency:\", corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e6126b",
   "metadata": {},
   "source": [
    "#### Create initial vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e01d9909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocab size: 26\n",
      "Initial vocab: ['e', 'y', 'u', 'v', 'd', 'p', 'h', 'o', 'n', 'b', 'l', 't', 'k', 'x', 'w', 's', 'i', 'g', 'f', 'z', 'a', 'r', 'j', 'c', 'm', 'q']\n"
     ]
    }
   ],
   "source": [
    "# Extract unique list of letters from corpus as initial vocab\n",
    "unique_letters = re.sub(r'[^a-zA-Z\\s]', '', ''.join(tokens))\n",
    "vocab = list(set(unique_letters))\n",
    "\n",
    "print(\"Initial vocab size:\", len(vocab))\n",
    "print(\"Initial vocab:\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33c01e9",
   "metadata": {},
   "source": [
    "### Implement bottom-up tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad3c771",
   "metadata": {},
   "source": [
    "#### Byte-pairing encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e26bccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to count frequency of adjacent symbol pairs\n",
    "def get_pair_stats(corpus):\n",
    "    pair_freq = defaultdict(int)\n",
    "    for symbols, freq in corpus:\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pair = (symbols[i], symbols[i + 1])\n",
    "            pair_freq[pair] += freq\n",
    "    return pair_freq\n",
    "\n",
    "# Define function to merge the most frequent pair in the corpus\n",
    "def merge_pair(pair, corpus):\n",
    "    a, b = pair\n",
    "    new_corpus = []\n",
    "    for symbols, freq in corpus:\n",
    "        new_syms = []\n",
    "        i = 0\n",
    "        while i < len(symbols):\n",
    "            if i < len(symbols)-1 and symbols[i] == a and symbols[i+1] == b:\n",
    "                new_syms.append(a+b)   # merge\n",
    "                i += 2\n",
    "            else:\n",
    "                new_syms.append(symbols[i])\n",
    "                i += 1\n",
    "        new_corpus.append((new_syms, freq))\n",
    "    return new_corpus\n",
    "\n",
    "# Main function to perform byte-pair encoding\n",
    "def byte_pair_encoding(corpus, vocab, num_merges=100):\n",
    "    \"\"\"\n",
    "    corpus: list of (word, freq) where word is a string.\n",
    "    vocab: initial vocab.\n",
    "    \"\"\"\n",
    "    # initialize corpus as list of (symbols, freq)\n",
    "    corpus = [(list(word), freq) for word, freq in corpus]\n",
    "\n",
    "    for _ in range(num_merges):\n",
    "        pair_freq = get_pair_stats(corpus)\n",
    "        if not pair_freq:\n",
    "            break\n",
    "        best_pair = max(pair_freq, key=pair_freq.get)   # most frequent\n",
    "        vocab.append(''.join(best_pair))                # add to vocab\n",
    "        corpus = merge_pair(best_pair, corpus)          # update corpus\n",
    "\n",
    "    return vocab, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d64f7bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE vocab size: 1026\n",
      "BPE vocab snippet: ['e', 'y', 'u', 'v', 'd', 'p', 'h', 'o', 'n', 'b', 'l', 't', 'k', 'x', 'w', 's', 'i', 'g', 'f', 'z', 'a', 'r', 'j', 'c', 'm', 'q', 'th', 'in', 're', 'the', 'on', 'at', 'an', 'er', 'en', 'al', 'st', 'or', 'ed', 'es', 'ion', 'of', 'and', 'ar', 'as', 'ic', 'it', 'ro', 'ing', 'is']\n",
      "['e', 'y', 'u', 'v', 'd', 'p', 'h', 'o', 'n', 'b', 'l', 't', 'k', 'x', 'w', 's', 'i', 'g', 'f', 'z', 'a', 'r', 'j', 'c', 'm', 'q', 'th', 'in', 're', 'the', 'on', 'at', 'an', 'er', 'en', 'al', 'st', 'or', 'ed', 'es', 'ion', 'of', 'and', 'ar', 'as', 'ic', 'it', 'ro', 'ing', 'is', 'ent', 'to', 'le', 'ch', 'ct', 'co', 'se', 've', 'ation', 'de', 'for', 'we', 'im', 'ly', 'ou', 'su', 'be', 'lo', 'ig', 'ce', 'ra', 'con', 'il', 'me', 'pro', 'ab', 'ol', 'di', 'res', 'ge', 'are', 'mp', 'te', 'un', 'mo', 'wi', 'ad', 'ver', 'ul', 'with', 'that', 'ment', 'wh', 'us', 'ci', 'ud', 'ter', 'ate', 'fe', 'ur', 'per', 'id', 'no', 'ex', 'ma', 'po', 'ity', 'ir', 'ive', 'pe', 'ac', 'ut', 'so', 'cl', 'ts', 'qu', 'ri', 'igh', 'stud', 'ne', 'ated', 'ach', 'sh', 'if', 'ren', 'ess', 'ical', 'by', 'ther', 'act', 'rom', 'were', 'ap', 'from', 'this', 'et', 'ffe', 'ents', 'ist', 'ers', 'ort', 'ant', 'cho', 'red', 'dat', 'was', 'am', 'comp', 'ag', 'par', 'ction', 'all', 'ure', 'age', 'ies', 'str', 'pl', 'ear', 'not', 'low', 'um', 'chool', 'iv', 'able', 'ass', 'gra', 'data', 'school', 'ha', 'el', 'port', 'ari', 'od', 'du', 'form', 'ions', 'ta', 'est', 'ations', 'cent', 'up', 'ys', 'ip', 'ho', 'rel', 'der', 'ms', 'vi', 'sa', 'high', 'out', 'our', 'iz', 'gen', 'om', 'cre', 'com', 'ls', 'ph', 'ine', 'cor', 'ase', 'reg', 'vari', 'ence', 'ign', 'more', 'dis', 'ates', 'bet', 'ich', 'have', 'tim', 'pre', 'students', 'ca', 'val', 'ance', 'these', 'cont', 'end', 'll', 'pres', 'part', 'ial', 'ke', 'ind', 'ong', 'vid', 'sit', 'sub', 'mode', 'ces', 'gro', 'diffe', 'ary', 'ob', 'li', 'tr', 'do', 'but', 'can', 'pr', 'he', 'differen', 'ss', 'year', 'ser', 'wor', 'inter', 'pa', 'which', 'one', 'than', 'other', 'uc', 'ific', 'ffect', 'leve', 'ty', 'use', 'ative', 'over', 'spe', 'fo', 'fic', 'ween', 'between', 'ill', 'bas', 'cal', 'em', 'sur', 'anal', 'their', 'gh', 'soci', 'ard', 'ual', 'ors', 'ber', 'sul', 'oun', 'chan', 'study', 'mat', 'bo', 'log', 'ans', 'ight', 'used', 'cted', 'ow', 'pon', 'ined', 'ast', 'ility', 'ics', 'incl', 'also', 'resul', 'sy', 'teach', 'des', 'man', 'arch', 'ay', 'tra', 'ous', 'ld', 'respon', 'ib', 'ational', 'effect', 'valu', 'time', 'cur', 'work', 'je', 'samp', 'sign', 'ition', 'col', 'has', 'ild', 'ult', 'pat', 'educ', 'coun', 'analys', 'percent', 'min', 'sion', 'fig', 'model', 'ree', 'incre', 'les', 'ds', 'speci', 'cess', 'ater', 'using', 'mon', 'serv', 'test', 'op', 'may', 'partic', 'there', 'each', 'search', 'tw', 'stru', 'ject', 'cons', 'show', 'who', 'level', 'bl', 'int', 'fir', 'meas', 'ust', 'bi', 'group', 'estim', 'cond', 'whe', 'nor', 'imp', 'research', 'ould', 'signific', 'ability', 'add', 'ures', 'fi', 'ally', 'cy', 'land', 'ict', 'num', 'they', 'child', 'ments', 'av', 'ound', 'includ', 'ef', 'its', 'based', 'tho', 'ens', 'af', 'colle', 'ities', 'two', 'class', 'velo', 'present', 'lar', 'ite', 'associ', 'stat', 'stand', 'ide', 'fol', 'significant', 'such', 'non', 'ia', 'under', 'produ', 'ory', 'most', 'been', 'fl', 'mal', 'udi', 'fact', 'cr', 'mar', 'develo', 'ev', 'results', 'table', 'education', 'develop', 'ced', 'analysis', 'first', 'fore', 'oc', 'appro', 'studi', 'gener', 'alth', 'ges', 'sim', 'inform', 'pt', 'different', 'ating', 'ning', 'ran', 'ack', 'only', 'some', 'follow', 'ice', 'ugh', 'report', 'inst', 'sed', 'med', 'lear', 'fa', 'peri', 'both', 'ric', 'meth', 'uring', 'tal', 'ross', 'ular', 'dist', 'ied', 'ile', 'number', 'method', 'sco', 'studies', 'ted', 'indi', 'quest', 'ins', 'ish', 'schools', 'particip', 'sci', 'far', 'sample', 'dep', 'rough', 'less', 'ris', 'increas', 'und', 'ically', 'observ', 'ident', 'about', 'posit', 'had', 'progra', 'gn', 'ribut', 'set', 'indic', 'pu', 'fin', 'will', 'au', 'char', 'when', 'na', 'information', 'sup', 'perform', 'ural', 'howe', 'comm', 'like', 'however', 'student', 'now', 'syst', 'assess', 'years', 'aver', 'during', 'ulation', 'where', 'pare', 'prob', 'surve', 'king', 'higher', 'those', 'pri', 'rain', 'temp', 'pred', 'ful', 'requ', 'second', 'tent', 'water', 'ting', 'well', 'cause', 'comple', 'go', 'gest', 'qual', 'ten', 'long', 'fer', 'ses', 'publ', 'would', 'trans', 'into', 'vol', 'ew', 'relation', 'rop', 'ages', 'found', 'national', 'addition', 'ship', 'gre', 'cer', 'through', 'struct', 'ep', 'individ', 'individual', 'import', 'ables', 'within', 'paren', 'view', 'subject', 'ough', 'cogn', 'cri', 'related', 'tain', 'ature', 'loc', 'ible', 'amp', 'children', 'ire', 'sp', 'mean', 'teachers', 'lin', 'new', 'ize', 'predict', 'la', 'ivity', 'because', 'ni', 'pect', 'ional', 'contro', 'associated', 'pop', 'ized', 'emp', 'state', 'gu', 'ants', 'rest', 'while', 'mis', 'ks', 'ared', 'term', 'tern', 'vir', 'ases', 'effects', 'statist', 'pla', 'ude', 'eng', 'respond', 'average', 'ateg', 'fun', 'ilar', 'similar', 'count', 'come', 'provid', 'itive', 'public', 'sm', 'survey', 'see', 'know', 'main', 'mult', 'process', 'led', 'large', 'gr', 'change', 'ients', 'across', 'grade', 'changes', 'acter', 'mic', 'find', 'mm', 'very', 'sele', 'olog', 'tex', 'inte', 'ings', 'differences', 'pol', 'ie', 'ited', 'ail', 'flu', 'cu', 'dise', 'character', 'cept', 'sing', 'levels', 'weight', 'function', 'after', 'total', 'sug', 'da', 'scal', 'vis', 'identif', 'hol', 'tre', 'figure', 'itut', 'variables', 'ient', 'three', 'suggest', 'typ', 'specific', 'wa', 'among', 'depend', 'ps', 'question', 'plic', 'measure', 'need', 'fiel', 'hy', 'system', 'coul', 'cus', 'could', 'xim', 'prof', 'graph', 'avail', 'ade', 'important', 'likely', 'rate', 'curren', 'clin', 'standard', 'inal', 'examp', 'any', 'vers', 'lement', 'net', 'models', 'iven', 'reported', 'impact', 'tech', 'cap', 'vement', 'themat', 'att', 'son', 'represent', 'exam', 'health', 'farm', 'cord', 'mathemat', 'ster', 'relative', 'compar', 'lower', 'response', 'cognitive', 'ract', 'science', 'same', 'section', 'many', 'rect', 'how', 'fur', 'iter', 'descri', 'ason', 'teacher', 'compared', 'ian', 'small', 'brain', 'disease', 'approach', 'cs', 'institut', 'values', 'further', 'aging', 'normal', 'consist', 'vide', 'risk', 'ency', 'sequ', 'including', 'ively', 'ward', 'err', 'als', 'maj', 'quality', 'vest', 'ices', 'mem', 'just', 'learning', 'groups', 'development', 'ak', 'ification', 'sen', 'read', 'face', 'lim', 'bal', 'sou', 'then', 'cip', 'uni', 'car', 'uro', 'increase', 'distribut', 'dif', 'example', 'period', 'available', 'vious', 'rates', 'ately', 'post', 'equ', 'type', 'ving', 'ress', 'value', 'ger', 'vent', 'ise', 'fra', 'exp', 'ctor', 'sea', 'ars', 'care', 'areas', 'current', 'tion', 'observed', 'design', 'population', 'ness', 'mi', 'area', 'app', 'factors', 'commun', 'support', 'loy', 'econ', 'sk', 'variable', 'included', 'ved', 'poss', 'due', 'provide', 'should', 'major', 'states', 'fam', 'potent', 'ences', 'evalu', 'program', 'achie', 'participants', 'characterist', 'performance', 'scores', 'org', 'relationship', 'econom', 'acade', 'comb', 'patients', 'potential', 'proble', 'impro', 'sin', 'mathematics', 'wn', 'stor', 'estimates', 'did', 'coast', 'prote', 'sever', 'sel', 'contin', 'os', 'experi', 'early', 'ily', 'comes', 'cul', 'gradu', 'eric', 'given', 'previous', 'sal', 'increased', 'particular', 'ask', 'shown', 'scale', 'ck', 'four', 'ute', 'effic', 'dr', 'art', 'mark', 'meter', 'subjects', 'centr', 'although', 'clinical', 'prim', 'invest', 'local', 'respect', 'ining', 'ref', 'pattern', 'size', 'abor', 'categ', 'ner', 'ext', 'employ', 'itud', 'cost', 'ization', 'ove', 'enro', 'sure', 'phys', 'br', 'ben', 'lev', 'affect', 'oper', 'ived', 'put', 'pract', 'tained', 'sol', 'thus', 'measures', 'ore', 'whether', 'positive', 'ator', 'deg', 'items', 'para', 'characteristics', 'mci', 'since', 'ources', 'get', 'college', 'surface', 'difference', 'regions', 'region', 'enroll', 'init', 'programs', 'ression', 'metr', 'fac', 'influ', 'being', 'istr', 'adv', 'methods', 'itions', 'several', 'oce', 'continu', 'control', 'degree', 'fall', 'them', 'issu', 'bed', 'polic']\n"
     ]
    }
   ],
   "source": [
    "# Implement BPE\n",
    "num_merges = 1000\n",
    "expanded_vocab, final_corpus = byte_pair_encoding(corpus, vocab, num_merges=num_merges)\n",
    "print(\"BPE vocab size:\", len(expanded_vocab))\n",
    "print(\"BPE vocab snippet:\", expanded_vocab[:50])\n",
    "print(expanded_vocab[:-50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fe73b5",
   "metadata": {},
   "source": [
    "# Compare with HuggingFace WordPiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa9fb1a",
   "metadata": {},
   "source": [
    "## Tokenization using WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4eee7c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "WordPiece vocab size: 1000\n",
      "WordPiece vocab snippet: [('pri', 728), ('##get', 842), ('through', 722), ('the', 60), ('patients', 956), ('trans', 535), ('wo', 564), ('nor', 721), ('pres', 504), ('##eld', 775), ('##ult', 481), ('new', 683), ('##tiv', 411), ('##ci', 816), ('##ly', 101), ('conf', 974), ('some', 516), ('##ub', 491), ('distrib', 885), ('period', 892), ('##vel', 174), ('##onom', 837), ('data', 206), ('pl', 413), ('total', 739), ('##ind', 399), ('##ame', 613), ('##ro', 76), ('scores', 953), ('aff', 851), ('go', 598), ('ro', 650), ('school', 208), ('##ilar', 698), ('b', 6), ('##ul', 98), ('perform', 595), ('me', 239), ('how', 381), ('##ency', 803), ('i', 13), ('##ass', 287), ('##ith', 117), ('with', 123), ('land', 776), ('cr', 690), ('##ss', 418), ('##ases', 544), ('effects', 691), ('character', 727)]\n",
      "[('y', 29), ('model', 281), ('some', 516), ('percent', 386), ('##ens', 294), ('sign', 365), ('bet', 269), ('##ol', 103), ('##uch', 372), ('rates', 906), ('provide', 928), ('colle', 813), ('##ew', 402), ('b', 6), ('##echn', 821), ('ma', 179), ('no', 429), ('a', 5), ('##orm', 197), ('multi', 771), ('throu', 636), ('##ution', 482), ('##ch', 225), ('inform', 498), ('commun', 937), ('qual', 795), ('##age', 230), ('cont', 233), ('total', 739), ('represent', 855), ('am', 353), ('higher', 600), ('brain', 818), ('##op', 211), ('ev', 407), ('mathematics', 959), ('sho', 300), ('##ased', 303), ('##ud', 106), ('##ur', 91), ('##onse', 809), ('aver', 596), ('##w', 46), ('national', 861), ('##ents', 171), ('ques', 568), ('##ect', 218), ('##di', 221), ('normal', 979), ('develop', 487)]\n"
     ]
    }
   ],
   "source": [
    "# Define a generator to yield single-token sentences for WordPiece tokenizer\n",
    "def corpus_iterator(corpus):\n",
    "    for word, freq in corpus:\n",
    "        for _ in range(freq):\n",
    "            yield [word]  # single-token sentence\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=True)\n",
    "tokenizer.train_from_iterator(\n",
    "    corpus_iterator(corpus),\n",
    "    vocab_size=1000,  # Set desired vocab size\n",
    "    min_frequency=1,  # Minimum frequency for a token to be included\n",
    ")\n",
    "\n",
    "print(\"WordPiece vocab size:\", tokenizer.get_vocab_size())\n",
    "print(\"WordPiece vocab snippet:\", list(tokenizer.get_vocab().items())[:50])\n",
    "print(list(tokenizer.get_vocab().items())[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7acaa93",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0948ab06",
   "metadata": {},
   "source": [
    "Comparing the developed BPE and WordPiece tokenizer, the difference in the token size relies on the hyperparamaters. In particular, vocab size for BPE tokenizer is dependent on the number of merges k while WordPiece tokenizer is dependent on the vocab size. Notably, the vocabulary from the token learner at lower number of merges or vocab size. The learner should have large iterations to give enough space to cover most frequent words. In this exercise, most of the vocab composition are characters and subwords."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
